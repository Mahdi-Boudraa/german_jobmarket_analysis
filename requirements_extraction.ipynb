{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "89vbNxAEjrU5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFUYFk-DbsbB",
    "outputId": "270fb496-fe12-46ab-b0ce-51a54ee4ae57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkillNer import failed, using alternative method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize models\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # Load before SkillNer\n",
    "\n",
    "# Correct SkillNer import (newer versions may use different import)\n",
    "try:\n",
    "    from skillNer.skill_extractor import SkillExtractor\n",
    "    skill_extractor = SkillExtractor(nlp)\n",
    "except ImportError:\n",
    "    # Fallback to alternative skill extraction\n",
    "    print(\"SkillNer import failed, using alternative method\")\n",
    "    skill_extractor = None\n",
    "\n",
    "# Initialize NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\",\n",
    "                       model=\"dslim/bert-base-NER\",\n",
    "                       aggregation_strategy=\"simple\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "o-NjFXqpjvYq"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional, Union\n",
    "\n",
    "# ----------------------\n",
    "# Constants\n",
    "# ----------------------\n",
    "SKILLS_LIST = {skill.lower() for skill in [\n",
    "    # --- Programming Languages ---\n",
    "    'python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'go', 'golang', 'ruby', 'php',\n",
    "    'swift', 'kotlin', 'rust', 'scala', 'r', 'dart', 'perl', 'bash', 'sql', 'html', 'css', 'Python',\n",
    "\n",
    "    # --- Web & Frontend ---\n",
    "    'react', 'angular', 'vue', 'svelte', 'next.js', 'nuxt.js', 'jquery', 'd3.js', 'redux',\n",
    "    'webpack', 'babel', 'tailwind', 'bootstrap', 'sass', 'less', 'graphql', 'rest', 'api',\n",
    "\n",
    "    # --- Backend & Frameworks ---\n",
    "    'node.js', 'express', 'django', 'flask', 'spring', 'spring boot', 'laravel', 'ruby on rails',\n",
    "    'asp.net', '.net core', 'fastapi', 'nestjs', 'hibernate', 'jpa', 'gin', 'echo',\n",
    "\n",
    "    # --- Mobile & Game Dev ---\n",
    "    'react native', 'flutter', 'xamarin', 'ionic', 'unity', 'unreal engine', 'cocos2d', 'godot',\n",
    "    'opengl', 'directx', 'phaser', 'pixi.js',\n",
    "\n",
    "    # --- DevOps & Cloud ---\n",
    "    'aws', 'azure', 'google cloud', 'gcp', 'docker', 'kubernetes', 'terraform', 'ansible',\n",
    "    'jenkins', 'github actions', 'gitlab ci', 'circleci', 'argo cd', 'helm', 'prometheus',\n",
    "    'grafana', 'splunk', 'new relic', 'datadog', 'istio', 'linkerd',\n",
    "\n",
    "    # --- Databases ---\n",
    "    'postgresql', 'mysql', 'mongodb', 'redis', 'cassandra', 'elasticsearch', 'dynamodb',\n",
    "    'firebase', 'snowflake', 'bigquery', 'oracle', 'sql server', 'mariadb', 'neo4j', 'cosmos db',\n",
    "\n",
    "    # --- Data Science & AI ---\n",
    "    'pandas', 'numpy', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'opencv', 'spark',\n",
    "    'hadoop', 'hive', 'kafka', 'airflow', 'mlflow', 'ray', 'nltk', 'spacy', 'hugging face',\n",
    "    'langchain', 'llama', 'gpt', 'bert', 'transformers', 'computer vision', 'nlp', 'ai'\n",
    "    'reinforcement learning', 'supervised learning', 'unsupervised learning', 'AI management'\n",
    "\n",
    "    # --- Data Engineering ---\n",
    "    'etl', 'elt', 'data pipeline', 'data warehouse', 'data lake', 'delta lake', 'apache beam',\n",
    "    'apache flink', 'dbt', 'prefect', 'dagster', 'redshift', 'databricks', 'tableau', 'power bi',\n",
    "    'looker', 'metabase', 'qlik', 'dax', 'power query', 'ssis', 'ssas', 'ssrs', 'data management'\n",
    "\n",
    "    # --- BI & Analytics ---\n",
    "    'tableau', 'power bi', 'looker', 'qlikview', 'qliksense', 'microstrategy', 'sisense',\n",
    "    'domo', 'matplotlib', 'seaborn', 'plotly', 'ggplot', 'excel', 'google sheets', 'vba',\n",
    "\n",
    "    # --- Cybersecurity ---\n",
    "    'owasp', 'penetration testing', 'metasploit', 'burp suite', 'wireshark', 'nmap',\n",
    "    'siem', 'soc', 'ids', 'ips', 'firewall', 'vpn', 'zero trust', 'saml', 'oauth', 'openid',\n",
    "    'iso 27001', 'gdpr', 'hipaa', 'pci dss', 'nist', 'cis controls',\n",
    "\n",
    "    # --- QA & Testing ---\n",
    "    'selenium', 'cypress', 'jest', 'mocha', 'junit', 'testng', 'pytest', 'postman',\n",
    "    'soapui', 'jmeter', 'loadrunner', 'appium', 'cucumber', 'specflow', 'robot framework',\n",
    "\n",
    "    # --- ERP & Business Systems ---\n",
    "    'sap', 'salesforce', 'microsoft dynamics', 'oracle erp', 'netsuite', 'workday', 'peoplesoft',\n",
    "    'sage', 'odoo', 'zoho', 'hubspot', 'microsoft power platform', 'sharepoint',\n",
    "\n",
    "    # --- IT Support & Admin ---\n",
    "    'active directory', 'windows server', 'linux', 'ubuntu', 'centos', 'red hat', 'macos',\n",
    "    'vmware', 'hyper-v', 'citrix', 'azure ad', 'office 365', 'exchange', 'powershell',\n",
    "\n",
    "    # --- Finance & Accounting ---\n",
    "    'quickbooks', 'xero', 'sage 50', 'peachtree', 'freshbooks', 'oracle financials',\n",
    "    'sap fico', 'hyperion', 'blackline', 'reconciliation', 'financial modeling', 'gaap',\n",
    "    'ifrs', 'taxation', 'audit', 'treasury', 'risk management',\n",
    "\n",
    "    # --- Project Management ---\n",
    "    'agile', 'scrum', 'kanban', 'saFe', 'waterfall', 'prince2', 'pmp', 'jira', 'trello',\n",
    "    'asana', 'monday.com', 'ms project', 'smartsheet', 'confluence', 'azure devops',\n",
    "\n",
    "    # --- Design & Creative ---\n",
    "    'figma', 'adobe xd', 'sketch', 'illustrator', 'photoshop', 'indesign', 'after effects',\n",
    "    'premiere pro', 'maya', 'blender', 'autocad', 'solidworks', 'revit', '3d modeling',\n",
    "\n",
    "    # --- Industry-Specific ---\n",
    "    # Healthcare\n",
    "    'hl7', 'fhir', 'epic', 'cerner', 'meditech', 'hipaa compliance', 'electronic health records',\n",
    "    # Manufacturing\n",
    "    'plc', 'scada', 'cnc', 'cad/cam', 'industry 4.0', 'iot', 'lean manufacturing', 'six sigma',\n",
    "    # Logistics\n",
    "    'wms', 'tms', 'supply chain', 'inventory management', 'sap mm', 'sap sd', 'sap ewm',\n",
    "    # Legal\n",
    "    'legal research', 'contract drafting', 'litigation', 'e-discovery', 'compliance', 'corporate law',\n",
    "    # Education\n",
    "    'lms', 'moodle', 'blackboard', 'e-learning', 'instructional design', 'curriculum development',\n",
    "    # Marketing\n",
    "    'seo', 'sem', 'ppc', 'google analytics', 'google tag manager', 'facebook ads', 'linkedin ads',\n",
    "    'marketing automation', 'hubspot', 'marketo', 'salesforce marketing cloud', 'content marketing',\n",
    "    'social media marketing', 'email marketing', 'crm', 'salesforce', 'zoho crm', 'hubspot crm','word',\n",
    "    'MS Office', 'outlook', 'ms office suite']}\n",
    "\n",
    "SKILL_CATEGORIES = {\n",
    "    # --- Programming Languages ---\n",
    "    'Programming Languages': [\n",
    "        'python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'go', 'golang',\n",
    "        'ruby', 'php', 'swift', 'kotlin', 'rust', 'scala', 'r', 'dart', 'perl',\n",
    "        'bash', 'html', 'css', 'Python', 'oop', 'object-oriented','object-oriented programming'\n",
    "    ],\n",
    "\n",
    "    # --- Web Development ---\n",
    "    'Web Development': [\n",
    "        'react', 'angular', 'vue', 'svelte', 'next.js', 'nuxt.js', 'jquery',\n",
    "        'd3.js', 'redux', 'webpack', 'babel', 'tailwind', 'bootstrap', 'sass',\n",
    "        'less', 'graphql', 'rest', 'api','web development', 'web dev','web developer'\n",
    "    ],\n",
    "\n",
    "    # --- Backend Development ---\n",
    "    'Backend Development': [\n",
    "        'node.js', 'express', 'django', 'flask', 'spring', 'spring boot',\n",
    "        'laravel', 'ruby on rails', 'asp.net', '.net core', 'fastapi', 'nestjs',\n",
    "        'hibernate', 'jpa', 'gin', 'echo','api', 'rest','rest api', 'restful'\n",
    "    ],\n",
    "\n",
    "    # --- Mobile & Game Development ---\n",
    "    'Mobile & Game Dev': [\n",
    "        'react native', 'flutter', 'xamarin', 'ionic', 'unity', 'unreal engine',\n",
    "        'cocos2d', 'godot', 'opengl', 'directx', 'phaser', 'pixi.js'\n",
    "    ],\n",
    "\n",
    "    # --- Cloud & DevOps ---\n",
    "    'Cloud & DevOps': [\n",
    "        'aws', 'azure', 'google cloud', 'gcp', 'docker', 'kubernetes',\n",
    "        'terraform', 'ansible', 'jenkins', 'github actions', 'gitlab ci',\n",
    "        'circleci', 'argo cd', 'helm', 'prometheus', 'grafana', 'splunk',\n",
    "        'new relic', 'datadog', 'istio', 'linkerd', 'cloud','cloud computing'\n",
    "    ],\n",
    "\n",
    "    # --- Databases ---\n",
    "    'Databases': [\n",
    "        'sql', 'postgresql', 'mysql', 'mongodb', 'redis', 'cassandra', 'databases',\n",
    "        'elasticsearch', 'dynamodb', 'firebase', 'snowflake', 'bigquery','relational',\n",
    "        'oracle', 'sql server', 'mariadb', 'neo4j', 'cosmos db', 'database','non relational'\n",
    "        'no sql','nosql','non-relational'\n",
    "    ],\n",
    "\n",
    "    # --- Data Science & AI ---\n",
    "    'Data Science & AI': [\n",
    "        'pandas', 'numpy', 'scikit-learn', 'tensorflow', 'pytorch', 'keras',\n",
    "        'opencv', 'spark', 'hadoop', 'hive', 'kafka', 'airflow', 'mlflow',\n",
    "        'ray', 'nltk', 'spacy', 'hugging face', 'langchain', 'llama', 'gpt',\n",
    "        'bert', 'transformers', 'computer vision', 'nlp', 'ai',\n",
    "        'reinforcement learning', 'supervised learning', 'unsupervised learning',\n",
    "        'ai management', 'ai developement', 'llm', 'deep learning', 'tensorflow',\n",
    "        'pytorch','data science',\n",
    "    ],\n",
    "\n",
    "    # --- Data Engineering ---\n",
    "    'Data Engineering': [\n",
    "        'etl', 'elt', 'data pipeline', 'data warehouse', 'data lake',\n",
    "        'delta lake', 'apache beam', 'apache flink', 'dbt', 'prefect',\n",
    "        'dagster', 'redshift', 'databricks', 'data strategy', 'data architecture',\n",
    "        'data foundation', 'data management', 'data engineering', 'data engineer'\n",
    "    ],\n",
    "\n",
    "    # --- BI & Analytics ---\n",
    "    'BI & Analytics': [\n",
    "        'tableau', 'power bi', 'looker', 'qlikview', 'qliksense', 'bi tools','bi'\n",
    "        'microstrategy', 'sisense', 'domo', 'matplotlib', 'seaborn', 'dashboards',\n",
    "        'plotly', 'ggplot', 'excel', 'google sheets', 'vba', 'pbi','dashboard',\n",
    "        'business intelligence','visualization', 'data analysis','data visualization'\n",
    "    ],\n",
    "\n",
    "    # --- Cybersecurity ---\n",
    "    'Cybersecurity': [\n",
    "        'owasp', 'penetration testing', 'metasploit', 'burp suite',\n",
    "        'wireshark', 'nmap', 'siem', 'soc', 'ids', 'ips', 'firewall',\n",
    "        'vpn', 'zero trust', 'saml', 'oauth', 'openid', 'iso 27001',\n",
    "        'gdpr', 'hipaa', 'pci dss', 'nist', 'cis controls'\n",
    "    ],\n",
    "\n",
    "    # --- QA & Testing ---\n",
    "    'QA & Testing': [\n",
    "        'selenium', 'cypress', 'jest', 'mocha', 'junit', 'testng',\n",
    "        'pytest', 'postman', 'soapui', 'jmeter', 'loadrunner',\n",
    "        'appium', 'cucumber', 'specflow', 'robot framework'\n",
    "    ],\n",
    "\n",
    "    # --- Business Systems ---\n",
    "    'Business Systems': [\n",
    "        'sap', 'salesforce', 'microsoft dynamics', 'oracle erp',\n",
    "        'netsuite', 'workday', 'peoplesoft', 'sage', 'odoo', 'zoho',\n",
    "        'hubspot', 'microsoft power platform', 'sharepoint'\n",
    "    ],\n",
    "\n",
    "    # --- IT & Administration ---\n",
    "    'IT & Admin': [\n",
    "        'active directory', 'windows server', 'linux', 'ubuntu',\n",
    "        'centos', 'red hat', 'macos', 'vmware', 'hyper-v', 'citrix',\n",
    "        'azure ad', 'office 365', 'exchange', 'powershell'\n",
    "    ],\n",
    "\n",
    "    # --- Business & Management ---\n",
    "    'Business & Management': [\n",
    "        'control models', 'business relationships', 'financial modeling',\n",
    "        'risk management', 'strategic planning', 'project management'\n",
    "    ],\n",
    "\n",
    "    # --- Design & Creative ---\n",
    "    'Design & Creative': [\n",
    "        'figma', 'adobe xd', 'sketch', 'illustrator', 'photoshop',\n",
    "        'indesign', 'after effects', 'premiere pro', 'maya', 'blender',\n",
    "        'autocad', 'solidworks', 'revit', '3d modeling'\n",
    "    ],\n",
    "\n",
    "    # --- Industry-Specific ---\n",
    "    'Healthcare IT': [\n",
    "        'hl7', 'fhir', 'epic', 'cerner', 'meditech', 'hipaa compliance',\n",
    "        'electronic health records'\n",
    "    ],\n",
    "    'Manufacturing': [\n",
    "        'plc', 'scada', 'cnc', 'cad/cam', 'industry 4.0', 'iot',\n",
    "        'lean manufacturing', 'six sigma'\n",
    "    ],\n",
    "    'Logistics': [\n",
    "        'wms', 'tms', 'supply chain', 'inventory management',\n",
    "        'sap mm', 'sap sd', 'sap ewm'\n",
    "    ],\n",
    "    'Marketing Tech': [\n",
    "        'seo', 'sem', 'ppc', 'google analytics', 'google tag manager',\n",
    "        'facebook ads', 'linkedin ads', 'marketing automation',\n",
    "        'content marketing', 'social media marketing', 'email marketing'\n",
    "    ],\n",
    "    'Office Tools': [\n",
    "        'word', 'ms office', 'outlook', 'ms office suite', 'powerpoint',\n",
    "        'excel', 'google docs', 'google sheets'\n",
    "    ]\n",
    "}\n",
    "\n",
    "SOFT_SKILLS = [\n",
    "    'team player', 'open-minded', 'organized', 'communication',\n",
    "    'adaptability', 'collaboration', 'initiative', 'creativity',\n",
    "    'analytical', 'problem solving', 'attention to detail',\n",
    "    'time management', 'leadership', 'management', 'communicate',\n",
    "    'communicates'\n",
    "]\n",
    "\n",
    "LANGUAGE_PATTERN = r'\\b(english|german|french|spanish|italian|dutch|mandarin|japanese)\\b'\n",
    "EXPERIENCE_PATTERN = r'(\\d+\\+?\\s*(?:years?|yrs?|months?)\\s*(?:of)?\\s*(?:experience|work)?)'\n",
    "EDUCATION_PATTERN = r'\\b(bachelor(?:\\'?s)?(?: of science)?|b\\.sc|master(?:\\'?s)?(?: of science)?|m\\.sc|ph\\.?d|diploma|degree|certification)\\b'\n",
    "EDUCATION_FIELD_PATTERN = r'(economic|technical|it[\\-\\s]related|computer science|engineering|business)'\n",
    "\n",
    "# ----------------------\n",
    "# Helper Functions\n",
    "# ----------------------\n",
    "def clean_entities(entities: List[Dict]) -> List[str]:\n",
    "    \"\"\"Post-process transformers NER output with compound skill handling.\"\"\"\n",
    "    cleaned = []\n",
    "    current_text = \"\"\n",
    "\n",
    "    for entity in entities:\n",
    "        text = entity['text']\n",
    "\n",
    "        # Handle word pieces\n",
    "        if text.startswith('##'):\n",
    "            current_text += text[2:]\n",
    "        else:\n",
    "            if current_text:\n",
    "                cleaned.append(current_text)\n",
    "            current_text = text\n",
    "\n",
    "        # Handle special compound cases\n",
    "        if current_text.lower() == 'ai' and cleaned and cleaned[-1].lower() == '/':\n",
    "            cleaned.pop()  # Remove the slash\n",
    "            current_text = 'AI/' + current_text\n",
    "\n",
    "    if current_text:\n",
    "        cleaned.append(current_text)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def get_skill_category(skill: str) -> str:\n",
    "    \"\"\"Categorize a skill based on predefined categories.\"\"\"\n",
    "    # Handle case where skill might be None or not a string\n",
    "    if not isinstance(skill, str):\n",
    "        return 'other'\n",
    "\n",
    "    try:\n",
    "        skill_lower = skill.lower()\n",
    "        for category, skills in SKILL_CATEGORIES.items():\n",
    "            # Compare against both raw and normalized versions\n",
    "            if (skill_lower in (s.lower() for s in skills) or\n",
    "                skill_lower in (normalize_skill(s).lower() for s in skills)):\n",
    "                return category\n",
    "        return 'other'\n",
    "    except AttributeError:\n",
    "        return 'other'\n",
    "\n",
    "def normalize_skill(skill: str) -> str:\n",
    "    \"\"\"Normalize skill names and handle variants.\"\"\"\n",
    "    skill = skill.lower()\n",
    "    variants = {\n",
    "        'ai': 'AI',\n",
    "        'ki': 'AI',\n",
    "        'it': 'IT',\n",
    "        'sql': 'SQL',\n",
    "        'ai/data strategy': 'AI/Data Strategy'\n",
    "    }\n",
    "    return variants.get(skill, skill.capitalize())\n",
    "\n",
    "def extract_skills(text: str, entities: List[Union[str, Dict]],\n",
    "                  extractor=None) -> Optional[List[Dict]]:\n",
    "    \"\"\"Enhanced skill extraction with comprehensive pattern matching and robust error handling.\"\"\"\n",
    "    skills = []\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # 1. Extract from skill extractor if available\n",
    "    if extractor:\n",
    "        try:\n",
    "            matches = extractor.annotate(text)\n",
    "            for match in matches.get('results', {}).get('full_matches', []):\n",
    "                if not isinstance(match, dict):\n",
    "                    continue\n",
    "\n",
    "                skill_name = match.get('doc_node_value')\n",
    "                if not isinstance(skill_name, str):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    skills.append({\n",
    "                        'skill': normalize_skill(skill_name),\n",
    "                        'type': 'hard',\n",
    "                        'category': get_skill_category(skill_name),\n",
    "                        'source': 'extractor',\n",
    "                        'confidence': float(match.get('score', 0.8))\n",
    "                    })\n",
    "                except (AttributeError, ValueError) as e:\n",
    "                    print(f\"Skipping invalid skill match: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skill extractor error: {e}\")\n",
    "\n",
    "    # 2. Extract from entities with validation\n",
    "    for ent in entities:\n",
    "        try:\n",
    "            if isinstance(ent, dict):\n",
    "                ent_text = ent.get('text', '')\n",
    "                confidence = float(ent.get('confidence', 0.8))\n",
    "            else:\n",
    "                ent_text = str(ent)\n",
    "                confidence = 0.9\n",
    "\n",
    "            if not ent_text:\n",
    "                continue\n",
    "\n",
    "            ent_lower = ent_text.lower()\n",
    "\n",
    "            if ent_lower in SKILLS_LIST:\n",
    "                skills.append({\n",
    "                    'skill': normalize_skill(ent_text),\n",
    "                    'type': 'hard',\n",
    "                    'category': get_skill_category(ent_text),\n",
    "                    'source': 'entity',\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        except (AttributeError, ValueError) as e:\n",
    "            print(f\"Skipping invalid entity: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 3. Enhanced compound skill patterns\n",
    "    compound_skills = [\n",
    "        # AI/Data Patterns\n",
    "        (r'\\b(ai|artificial intelligence|ml|machine learning|data)\\s*(strategy|roadmap|vision)\\b',\n",
    "         'AI/Data Strategy'),\n",
    "        (r'\\bdata\\s+(foundation|governance|management|architecture|platform)\\b',\n",
    "         'Data Foundation'),\n",
    "        (r'\\bdata\\s+(templates?|models?|schemas?)\\b',\n",
    "         'Data Modeling'),\n",
    "        (r'\\b(regulatory|compliance)\\s+data\\b',\n",
    "         'Regulatory Data'),\n",
    "\n",
    "        # Cloud & Infrastructure\n",
    "        (r'\\bcloud\\b.*\\b(computing|architecture|services|migration|native)\\b',\n",
    "         'Cloud Computing'),\n",
    "        (r'\\b(on[-\\s]*premise|on[-\\s]*prem)\\b',\n",
    "         'On-Premise Systems'),\n",
    "        (r'\\b(hybrid|multi[-\\s]*cloud)\\b',\n",
    "         'Hybrid/Multi Cloud'),\n",
    "        (r'\\b(kubernetes|k8s)\\b',\n",
    "         'Kubernetes'),\n",
    "        (r'\\b(docker|containerization|containers)\\b',\n",
    "         'Containerization'),\n",
    "        (r'\\b(cicd|ci/cd|continuous integration|continuous deployment)\\b',\n",
    "         'CI/CD'),\n",
    "\n",
    "        # Cloud Providers\n",
    "        (r'\\b(aws|azure|gcp|google cloud platform)\\b',\n",
    "         lambda m: m.group(1).upper()),\n",
    "\n",
    "        # Programming Languages\n",
    "        (r'\\b(python|java|scala|r|sql|go|typescript|javascript|c\\+\\+|c#|bash|shell|perl|php|rust|ruby|matlab)\\b',\n",
    "         'Programming Language'),\n",
    "\n",
    "        # Data Engineering\n",
    "        (r'\\b(etl|elt|data\\s*pipeline|airflow|data\\s*integration|data\\s*orchestration)\\b',\n",
    "         'Data Engineering'),\n",
    "        (r'\\bdbt\\b',\n",
    "         'dbt (Data Build Tool)'),\n",
    "        (r'\\b(nifi|luigi|kafka|flink|beam|streamsets|azkaban)\\b',\n",
    "         'Data Pipeline Tool'),\n",
    "\n",
    "        # DevOps/MLOps\n",
    "        (r'\\b(devops|mlops|jenkins|gitlab\\s*ci|argo\\s*cd|kubeflow|mlflow|dvc|ml\\s+ops)\\b',\n",
    "         'DevOps/MLOps'),\n",
    "        (r'\\b(infrastructure\\s+as\\s+code|terraform|ansible|cloudformation|pulumi)\\b',\n",
    "         'IaC'),\n",
    "\n",
    "        # Data Visualization\n",
    "        (r'\\b(data|business)\\s*dashboards?\\b',\n",
    "         'Data Dashboards'),\n",
    "        (r'\\b(power\\s*bi|tableau|looker|qlik|superset|metabase|microstrategy|google\\s*data\\s*studio)\\b',\n",
    "         'BI Tools'),\n",
    "\n",
    "        # Data Science\n",
    "        (r'\\b(statistical\\s+analysis|predictive\\s+modeling|data\\s+science|descriptive\\s+analytics)\\b',\n",
    "         'Data Science'),\n",
    "        (r'\\b(customer\\s+segmentation|churn\\s+prediction|forecasting|recommendation\\s+system)\\b',\n",
    "         'Use Case'),\n",
    "        (r'\\b(a/b\\s+testing|causal\\s+inference|time\\s+series)\\b',\n",
    "         'Analytics Techniques'),\n",
    "\n",
    "        # Machine Learning\n",
    "        (r'\\b(supervised|unsupervised|reinforcement)\\s+learning\\b',\n",
    "         'Machine Learning Type'),\n",
    "        (r'\\b(classification|regression|clustering|nlp|computer\\s*vision|deep\\s*learning)\\b',\n",
    "         'ML Techniques'),\n",
    "        (r'\\b(pytorch|tensorflow|sklearn|scikit-learn|keras|xgboost|lightgbm)\\b',\n",
    "         'ML/AI Framework'),\n",
    "\n",
    "        # Big Data\n",
    "        (r'\\b(hadoop|spark|pyspark|hive|pig|flink|beam|storm)\\b',\n",
    "         'Big Data Tool'),\n",
    "\n",
    "        # Databases\n",
    "        (r'\\b(sql|nosql|relational\\s+database|mongodb|postgres|mysql|oracle|sqlite|mariadb)\\b',\n",
    "         'Database'),\n",
    "        (r'\\b(data\\s*warehouse|dwh|snowflake|bigquery|redshift|databricks|synapse)\\b',\n",
    "         'Data Warehouse'),\n",
    "        (r'\\b(data\\s+lake|lakehouse)\\b',\n",
    "         'Data Lake')\n",
    "    ]\n",
    "\n",
    "    # Process compound patterns\n",
    "    for pattern, handler in compound_skills:\n",
    "        try:\n",
    "            for match in re.finditer(pattern, text_lower):\n",
    "                if callable(handler):\n",
    "                    skill_name = handler(match)\n",
    "                else:\n",
    "                    skill_name = handler\n",
    "\n",
    "                if isinstance(skill_name, str):\n",
    "                    skills.append({\n",
    "                        'skill': skill_name,\n",
    "                        'type': 'hard',\n",
    "                        'category': get_skill_category(skill_name),\n",
    "                        'source': 'pattern',\n",
    "                        'confidence': 0.95\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pattern {pattern}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 4. Remove duplicates (keeping highest confidence version)\n",
    "    unique_skills = {}\n",
    "    for skill in skills:\n",
    "        try:\n",
    "            if not isinstance(skill, dict):\n",
    "                continue\n",
    "\n",
    "            name_lower = str(skill.get('skill', '')).lower()\n",
    "            if not name_lower:\n",
    "                continue\n",
    "\n",
    "            current_conf = float(skill.get('confidence', 0))\n",
    "\n",
    "            if (name_lower not in unique_skills or\n",
    "                unique_skills[name_lower].get('confidence', 0) < current_conf):\n",
    "                unique_skills[name_lower] = skill\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping invalid skill entry: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to clean output format\n",
    "    clean_skills = []\n",
    "    for skill in unique_skills.values():\n",
    "        try:\n",
    "            clean_skills.append({\n",
    "                'skill': str(skill.get('skill', '')),\n",
    "                'skill_type': str(skill.get('type', 'hard')),\n",
    "                'skill_category': str(skill.get('category', 'other')),\n",
    "                'detection_method': str(skill.get('source', 'unknown')),\n",
    "                'confidence_score': float(skill.get('confidence', 0.5))\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping malformed skill: {e}\")\n",
    "            continue\n",
    "\n",
    "    return clean_skills or None\n",
    "\n",
    "def extract_education(text: str) -> Dict:\n",
    "    \"\"\"Enhanced education extraction with degree types and fields.\"\"\"\n",
    "    degrees = list(set(re.findall(EDUCATION_PATTERN, text, re.IGNORECASE)))\n",
    "    fields = list(set(re.findall(EDUCATION_FIELD_PATTERN, text.lower())))\n",
    "\n",
    "    if not degrees and not fields:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'degrees': [d.lower() for d in degrees] if degrees else None,\n",
    "        'fields': [f.capitalize() for f in fields] if fields else None\n",
    "    }\n",
    "\n",
    "def extract_soft_skills(text: str) -> Optional[List[Dict]]:\n",
    "    \"\"\"Enhanced soft skill extraction with normalization.\"\"\"\n",
    "    found = []\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    for skill in SOFT_SKILLS:\n",
    "        skill_lower = skill.lower()\n",
    "        if skill_lower in text_lower:\n",
    "            # Normalize verb forms to nouns\n",
    "            normalized = skill_lower\n",
    "            if skill_lower.endswith('s') and skill_lower[:-1] in SOFT_SKILLS:\n",
    "                normalized = skill_lower[:-1]\n",
    "            elif skill_lower.endswith('e') and skill_lower + 's' in SOFT_SKILLS:\n",
    "                normalized = skill_lower + 's'\n",
    "\n",
    "            found.append({\n",
    "                'name': normalized.capitalize(),\n",
    "                'type': 'soft_skill'\n",
    "            })\n",
    "\n",
    "    return found or None\n",
    "\n",
    "# for cleaning the result dict before assigning it to df\n",
    "def clean_extraction_results(results: dict) -> dict:\n",
    "    \"\"\"Cleans the raw extraction results and returns a structured dict with normalized fields.\"\"\"\n",
    "\n",
    "    cleaned = {\n",
    "        'hard_skills': {},      # {skill: category}\n",
    "        'languages': None,      # List of languages\n",
    "        'experience': None,     # First experience string\n",
    "        'education': None,      # Dict: {degree/field: type}\n",
    "        'soft_skills': None     # List of soft skill names\n",
    "    }\n",
    "\n",
    "    # --- Hard Skills ---\n",
    "    if isinstance(results.get('skills'), list):\n",
    "        for skill in results['skills']:\n",
    "            name = skill.get('skill', '').strip().lower()\n",
    "            category = skill.get('skill_category', '').strip().lower()\n",
    "            if name and category:\n",
    "                cleaned['hard_skills'][name] = category\n",
    "    elif isinstance(results.get('skills'), dict):\n",
    "        # If already in dict format\n",
    "        cleaned['hard_skills'] = {\n",
    "            k.strip().lower(): v.strip().lower()\n",
    "            for k, v in results['skills'].items()\n",
    "        }\n",
    "\n",
    "    # --- Languages ---\n",
    "    if isinstance(results.get('languages'), list):\n",
    "        languages = [lang.strip().lower() for lang in results['languages'] if isinstance(lang, str)]\n",
    "        cleaned['languages'] = list(set(languages)) or None\n",
    "\n",
    "    # --- Experience ---\n",
    "    if isinstance(results.get('experience'), list) and results['experience']:\n",
    "        cleaned['experience'] = results['experience'][0].strip()\n",
    "\n",
    "    # --- Education ---\n",
    "    if isinstance(results.get('education'), dict):\n",
    "        degrees = results['education'].get('degrees', [])\n",
    "        fields = results['education'].get('fields', [])\n",
    "        if (degrees is None) | (fields is None):\n",
    "          cleaned['education'] = None\n",
    "        else:\n",
    "          combined = {\n",
    "            deg.strip().lower(): 'degree'\n",
    "            for deg in degrees if isinstance(deg, str)\n",
    "          }\n",
    "          combined.update({\n",
    "            fld.strip().lower(): 'field'\n",
    "            for fld in fields if isinstance(fld, str)\n",
    "          })\n",
    "          cleaned['education'] = combined or None\n",
    "\n",
    "    # --- Soft Skills ---\n",
    "    if isinstance(results.get('soft_skills'), list):\n",
    "        softs = [s.get('name', '').strip().lower() for s in results['soft_skills'] if isinstance(s, dict) and s.get('name')]\n",
    "        cleaned['soft_skills'] = list(set(softs)) or None\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Main Extraction Function\n",
    "# ----------------------\n",
    "\n",
    "def extract_job_requirements(text: str, ner_pipeline = ner_pipeline, skill_extractor=None) -> Dict:\n",
    "    \"\"\"Enhanced job requirements extraction with structured output.\"\"\"\n",
    "    results = {\n",
    "        'skills': None,\n",
    "        'languages': None,\n",
    "        'experience': None,\n",
    "        'education': None,\n",
    "        'soft_skills': None,\n",
    "        'entities': None,\n",
    "    }\n",
    "\n",
    "    # Pre-process text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = text.replace('AI / Data', 'AI/Data').replace('AI/ Data', 'AI/Data')\n",
    "\n",
    "    # Named Entity Recognition\n",
    "    try:\n",
    "        raw_entities = ner_pipeline(text)\n",
    "        filtered = [\n",
    "            {'text': e['word'], 'type': e['entity_group'], 'confidence': float(e['score'])}\n",
    "            for e in raw_entities if e['entity_group'] != 'PER'\n",
    "        ]\n",
    "        cleaned_entities = clean_entities(filtered)\n",
    "        results['entities'] = cleaned_entities or None\n",
    "    except Exception as e:\n",
    "        print(f\"NER pipeline failed: {e}\")\n",
    "        cleaned_entities = []\n",
    "\n",
    "    # Extract each category with enhanced processing\n",
    "    results['skills'] = extract_skills(text, cleaned_entities, skill_extractor)\n",
    "    results['languages'] = extract_languages(text)\n",
    "    results['experience'] = extract_experience(text)\n",
    "    results['education'] = extract_education(text)\n",
    "    results['soft_skills'] = extract_soft_skills(text)\n",
    "\n",
    "\n",
    "    return clean_extraction_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "Ty78lLlaBPLY"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('job_postings_classified.csv')\n",
    "df = df.drop(df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "nIyfc3rfrmDG"
   },
   "outputs": [],
   "source": [
    "# Convert all values to strings first\n",
    "df[\"requirements_extracted\"] = df[\"requirements\"].astype(str).apply(extract_job_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "ogWpXkzXCVX2"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"jobs_with_requirements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9OBSabOR57C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
